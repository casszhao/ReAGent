{
    "$schema": "../docs/rationalization.schema.json",
    "id": 101,
    "input-text": [
        "I",
        " knew",
        " it",
        " was",
        " low",
        ",",
        " but",
        " that",
        "'s",
        " before",
        " I",
        " saw",
        " it",
        " in",
        " person",
        ".",
        " (",
        "Just",
        " then",
        " I",
        " thought",
        " about",
        " my",
        " ex",
        "-",
        "wife",
        ",",
        " but",
        " I",
        " had",
        " to",
        " stop",
        " thinking",
        " about",
        " her",
        ").",
        " When",
        " I",
        " did",
        " end",
        " up",
        " seeing",
        " it",
        " in",
        " person",
        ",",
        " it",
        " was",
        " even"
    ],
    "input-tokens": [
        40,
        2993,
        340,
        373,
        1877,
        11,
        475,
        326,
        338,
        878,
        314,
        2497,
        340,
        287,
        1048,
        13,
        357,
        5703,
        788,
        314,
        1807,
        546,
        616,
        409,
        12,
        22095,
        11,
        475,
        314,
        550,
        284,
        2245,
        3612,
        546,
        607,
        737,
        1649,
        314,
        750,
        886,
        510,
        4379,
        340,
        287,
        1048,
        11,
        340,
        373,
        772
    ],
    "target-text": " lower",
    "target-token": 2793,
    "rational-size": 9,
    "rational-positions": [
        3,
        4,
        6,
        11,
        18,
        24,
        41,
        47,
        48
    ],
    "rational-text": [
        " was",
        " low",
        " but",
        " saw",
        " then",
        "-",
        " seeing",
        " was",
        " even"
    ],
    "rational-tokens": [
        373,
        1877,
        475,
        2497,
        788,
        12,
        4379,
        373,
        772
    ],
    "importance-scores": [
        1.4579672191761262e-10,
        1.884479949865181e-08,
        1.495590566946703e-07,
        0.30827319622039795,
        0.30235913395881653,
        7.485679816454649e-06,
        0.011654374189674854,
        1.2315181265876163e-05,
        2.541808603873097e-11,
        5.008531047678844e-07,
        0.00013283929729368538,
        0.016076840460300446,
        0.014808012172579765,
        1.8314966609978e-07,
        9.686808880360331e-06,
        2.648019803430657e-09,
        0.003607075661420822,
        2.1665072622197024e-13,
        0.11105546355247498,
        1.4465330195889692e-06,
        6.956268407520838e-06,
        7.139640365494415e-05,
        9.104389464553719e-11,
        8.544834173562776e-09,
        0.0014110702322795987,
        0.00014104558795224875,
        4.170038536699394e-09,
        1.7619173320326809e-07,
        7.639052910235478e-07,
        1.1109964361821767e-05,
        3.987982495345932e-07,
        3.747953087440692e-05,
        4.782413816428743e-05,
        6.671264145552414e-07,
        1.0762223610072397e-05,
        1.9074792093065618e-11,
        9.059099852493091e-07,
        3.975070272943526e-11,
        1.4129113157235196e-10,
        2.397013476596044e-09,
        2.751403940237651e-07,
        0.0008222672622650862,
        5.660517217620509e-06,
        1.3379559682391573e-09,
        1.0298875167791266e-05,
        6.232950545381755e-05,
        1.6743157971177425e-09,
        5.3658468459616415e-06,
        0.22935451567173004
    ],
    "comments": {
        "created-by": "run_analogies.py",
        "args": {
            "model": "gpt2-xl",
            "cache_dir": "cache/",
            "tokenizer": "gpt2-xl",
            "data_dir": "data/analogies/gpt2_xl",
            "importance_results_dir": "rationalization_results/analogies/gpt2_xl_ours/top3_replace0.3_max3000_batch10",
            "device": "cuda",
            "rationalization_config": "config//top3_replace0.3_max3000_batch10.json",
            "input_num_ratio": 1.0,
            "logfolder": "logs/analogies/gpt2_xl_ours/top3_replace0.3_max3000_batch10",
            "loglevel": 20
        },
        "time_elapsed": 769.6200904846191,
        "separate_rational": [
            [
                " low",
                " even",
                " saw",
                " (",
                ","
            ],
            [
                " even",
                " low",
                "-",
                " but",
                " thinking"
            ],
            [
                " was",
                " even",
                " low",
                "wife",
                " but"
            ],
            [
                " even",
                " it",
                " person",
                " low",
                ","
            ],
            [
                " low",
                " but",
                "-",
                " seeing",
                " even"
            ],
            [
                " was",
                " low",
                " about",
                " was",
                " even"
            ],
            [
                " was",
                " seeing",
                " low",
                " it",
                " was"
            ],
            [
                " low",
                " it",
                " When",
                ".",
                "'s"
            ],
            [
                " low",
                " that",
                " saw",
                " even",
                " then"
            ],
            [
                " then",
                ",",
                " end",
                " low",
                " even"
            ]
        ]
    }
}