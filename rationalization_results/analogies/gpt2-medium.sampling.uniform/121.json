{
    "$schema": "../docs/rationalization.schema.json",
    "id": 121,
    "input-text": [
        "I",
        " had",
        " never",
        " been",
        " friends",
        " with",
        " any",
        " Chinese",
        " people",
        " before",
        ".",
        " (",
        "The",
        " funn",
        "iest",
        " thing",
        " happened",
        " to",
        " me",
        " the",
        " other",
        " day",
        ",",
        " but",
        " that",
        "'s",
        " a",
        " story",
        " for",
        " another",
        " time",
        ").",
        " In",
        " fact",
        ",",
        " I",
        " had",
        " never",
        " even",
        " been",
        " to"
    ],
    "input-tokens": [
        40,
        550,
        1239,
        587,
        2460,
        351,
        597,
        3999,
        661,
        878,
        13,
        357,
        464,
        36090,
        6386,
        1517,
        3022,
        284,
        502,
        262,
        584,
        1110,
        11,
        475,
        326,
        338,
        257,
        1621,
        329,
        1194,
        640,
        737,
        554,
        1109,
        11,
        314,
        550,
        1239,
        772,
        587,
        284
    ],
    "target-text": " China",
    "target-token": 2807,
    "rational-size": 5,
    "rational-positions": [
        37,
        5,
        39,
        13,
        40
    ],
    "rational-text": [
        " never",
        " with",
        " been",
        " funn",
        " to"
    ],
    "rational-tokens": [
        1239,
        351,
        587,
        36090,
        284
    ],
    "importance-scores": [
        1.8411629058510227e-19,
        1.2627731153298851e-20,
        1.3350848115019189e-17,
        4.2896045058342465e-29,
        1.5271161071805928e-13,
        0.01053487416356802,
        1.1315118370422857e-18,
        5.211494755706647e-13,
        4.246977202575154e-22,
        1.3553788508373058e-34,
        7.914808826812063e-22,
        3.538053877649716e-11,
        1.293753594014571e-22,
        7.37852778875947e-10,
        3.4035089347697133e-28,
        1.2519756214482621e-21,
        1.150247099254897e-26,
        2.4453903019704113e-31,
        3.034655839924625e-30,
        4.8753917466004225e-29,
        2.7576507556623434e-16,
        1.9126615519023293e-32,
        7.242778820133884e-31,
        1.2418742178426909e-26,
        7.96532842938999e-15,
        3.7382094876512745e-20,
        2.4945909775380227e-24,
        2.5893067454147413e-25,
        6.661183854145e-27,
        9.325904993356651e-26,
        1.7843029243447978e-29,
        6.300895453145489e-17,
        1.3995355115444152e-25,
        3.1951808820290806e-12,
        2.162505306432587e-18,
        2.587711403663484e-14,
        5.769292815136346e-24,
        0.989464521408081,
        1.0385748898970794e-19,
        5.588320277638559e-07,
        2.2502034957572192e-10
    ],
    "comments": {
        "created-by": "run_analogies.py",
        "args": {
            "model": "gpt2-medium",
            "tokenizer": "gpt2-medium",
            "data_dir": "data/analogies",
            "output_dir": "rationalization_results/analogies/gpt2-medium.sampling.uniform",
            "device": "cuda",
            "rationalization_config": "config/test2.json",
            "input_data_size": 1,
            "logfile": "logs/analogies/gpt2-medium.sampling.uniform/test.log",
            "loglevel": 20
        },
        "time_elapsed": 7.038435697555542
    }
}