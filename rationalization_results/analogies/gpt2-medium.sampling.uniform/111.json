{
    "$schema": "../docs/rationalization.schema.json",
    "id": 111,
    "input-text": [
        "I",
        " thought",
        " it",
        " would",
        " be",
        " the",
        " smartest",
        " thing",
        " I",
        "'d",
        " ever",
        " encounter",
        ".",
        " (",
        "I",
        " tried",
        " to",
        " ignore",
        " my",
        " phone",
        " vibr",
        "ating",
        " in",
        " my",
        " pocket",
        ").",
        " But",
        " when",
        " I",
        " did",
        " end",
        " up",
        " encountering",
        " it",
        ",",
        " it",
        " turned",
        " out",
        " it",
        " wasn",
        "'t",
        " so"
    ],
    "input-tokens": [
        40,
        1807,
        340,
        561,
        307,
        262,
        44730,
        1517,
        314,
        1549,
        1683,
        8791,
        13,
        357,
        40,
        3088,
        284,
        8856,
        616,
        3072,
        12611,
        803,
        287,
        616,
        10000,
        737,
        887,
        618,
        314,
        750,
        886,
        510,
        42398,
        340,
        11,
        340,
        2900,
        503,
        340,
        2492,
        470,
        523
    ],
    "target-text": " smart",
    "target-token": 4451,
    "rational-size": 5,
    "rational-positions": [
        13,
        41,
        40,
        27,
        28
    ],
    "rational-text": [
        " (",
        " so",
        "'t",
        " when",
        " I"
    ],
    "rational-tokens": [
        357,
        523,
        470,
        618,
        314
    ],
    "importance-scores": [
        3.86416056258803e-28,
        1.9247678908386202e-16,
        2.574168100899499e-10,
        9.612271897166457e-22,
        4.602977166090483e-10,
        2.1499577245570005e-21,
        2.320706120491605e-14,
        2.258282619324037e-28,
        4.010224948092754e-26,
        2.57957547882805e-17,
        2.0124317179676355e-20,
        5.999276455697484e-23,
        1.1289835169167595e-13,
        0.9988083839416504,
        1.4748302758297912e-17,
        2.746242632172685e-13,
        9.208553838019985e-22,
        3.5444670202361417e-12,
        4.0529870920859338e-22,
        2.1914559495838406e-16,
        2.091669300938637e-15,
        1.113048125994631e-24,
        4.281407228745593e-09,
        3.163670642934226e-19,
        1.522152219882766e-16,
        1.7245086556796143e-16,
        4.877051019980261e-12,
        1.6011966863516136e-06,
        3.99982837961943e-07,
        7.482698055769554e-16,
        3.247423326688997e-30,
        1.5522252713174565e-26,
        8.754988921333418e-12,
        1.3371826432587164e-12,
        2.0881268936179254e-10,
        1.118928890772132e-21,
        3.521154063743092e-19,
        1.6170718332867603e-27,
        8.192534439797704e-23,
        1.7587827431301607e-24,
        8.421222446486354e-06,
        0.0011813215678557754
    ],
    "comments": {
        "created-by": "run_analogies.py",
        "args": {
            "model": "gpt2-medium",
            "tokenizer": "gpt2-medium",
            "data_dir": "data/analogies",
            "output_dir": "rationalization_results/analogies/gpt2-medium.sampling.uniform",
            "device": "cuda",
            "rationalization_config": "config/test2.json",
            "input_data_size": 1,
            "logfile": "logs/analogies/gpt2-medium.sampling.uniform/test.log",
            "loglevel": 20
        },
        "time_elapsed": 16.51374316215515
    }
}