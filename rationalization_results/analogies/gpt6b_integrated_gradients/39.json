{
    "$schema": "../docs/rationalization.schema.json",
    "id": 39,
    "input-text": [
        "I",
        " had",
        " never",
        " been",
        " friends",
        " with",
        " any",
        " Korean",
        " people",
        " before",
        ".",
        " (",
        "The",
        " funn",
        "iest",
        " thing",
        " happened",
        " to",
        " me",
        " the",
        " other",
        " day",
        ",",
        " but",
        " that",
        "'s",
        " a",
        " story",
        " for",
        " another",
        " time",
        ").",
        " In",
        " fact",
        ",",
        " I",
        " had",
        " never",
        " even",
        " been",
        " to"
    ],
    "input-tokens": [
        40,
        550,
        1239,
        587,
        2460,
        351,
        597,
        6983,
        661,
        878,
        13,
        357,
        464,
        36090,
        6386,
        1517,
        3022,
        284,
        502,
        262,
        584,
        1110,
        11,
        475,
        326,
        338,
        257,
        1621,
        329,
        1194,
        640,
        737,
        554,
        1109,
        11,
        314,
        550,
        1239,
        772,
        587,
        284
    ],
    "target-text": " Korea",
    "target-token": 4969,
    "rational-size": 5,
    "rational-positions": [
        0,
        40,
        39,
        13,
        7
    ],
    "rational-text": [
        "I",
        " to",
        " been",
        " funn",
        " Korean"
    ],
    "rational-tokens": [
        40,
        284,
        587,
        36090,
        6983
    ],
    "importance-scores": [
        0.9995749592781067,
        4.021559085742865e-09,
        2.4964585065845313e-08,
        2.4581106927712426e-08,
        4.305496048573332e-08,
        6.274617492252332e-10,
        3.2848266329210674e-08,
        7.34350535935846e-08,
        2.577601598829915e-08,
        1.4448723106852412e-08,
        4.00910593612025e-09,
        1.1857141046789366e-08,
        5.7322111501889594e-08,
        8.616039082198768e-08,
        4.81676067920489e-08,
        3.1166205616273146e-08,
        2.8702727306040288e-08,
        6.390899809360917e-09,
        1.6238042022109767e-08,
        1.2681899974609223e-08,
        2.0555566138114045e-08,
        2.1924442705767433e-08,
        1.4826050609428876e-08,
        1.9918084959158477e-08,
        1.5493473171090955e-08,
        1.6121822099535166e-08,
        3.197473930072192e-08,
        2.0509355991293887e-08,
        1.9172215814933224e-08,
        2.1003611294645452e-08,
        2.4028729228575685e-08,
        2.7537778279906888e-08,
        5.595506280542395e-08,
        2.4067166037866627e-08,
        1.2818059502706092e-08,
        2.111708852226002e-08,
        1.7943937891118367e-08,
        2.1152443352434602e-08,
        3.338369225502902e-08,
        3.3414733024983434e-07,
        0.0004238167603034526
    ],
    "comments": {
        "created-by": "run_analogies.py",
        "args": {
            "model": "EleutherAI/gpt-j-6b",
            "cache_dir": "./cache/",
            "tokenizer": "EleutherAI/gpt-j-6b",
            "data_dir": "data/analogies/gpt6b/",
            "importance_results_dir": "rationalization_results/analogies/gpt6b_integrated_gradients",
            "device": "cuda",
            "rationalization_config": "config/eva_integrated_gradients.json",
            "input_num_ratio": 1,
            "logfolder": "logs/analogies/gpt6b_integrated_gradients",
            "loglevel": 20
        },
        "time_elapsed": 14.609083890914917
    }
}