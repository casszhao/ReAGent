{
    "$schema": "../docs/rationalization.schema.json",
    "id": 34,
    "input-text": [
        "</s>",
        "I",
        " can",
        " usually",
        " work",
        " by",
        " myself",
        ".",
        " (",
        "I",
        " was",
        " so",
        " behind",
        " on",
        " work",
        " but",
        " I",
        " tried",
        " to",
        " distract",
        " myself",
        ").",
        " Although",
        " it",
        "'s",
        " so",
        " much",
        " better",
        " when",
        " someone",
        " else",
        " also"
    ],
    "input-tokens": [
        2,
        100,
        64,
        2333,
        173,
        30,
        2185,
        4,
        36,
        100,
        21,
        98,
        639,
        15,
        173,
        53,
        38,
        1381,
        7,
        21943,
        2185,
        322,
        2223,
        24,
        18,
        98,
        203,
        357,
        77,
        951,
        1493,
        67
    ],
    "target-text": " works",
    "target-token": 1364,
    "rational-size": 5,
    "rational-positions": [
        0,
        28,
        1,
        31,
        29
    ],
    "rational-text": [
        "</s>",
        " when",
        "I",
        " also",
        " someone"
    ],
    "rational-tokens": [
        2,
        77,
        100,
        67,
        951
    ],
    "importance-scores": [
        0.07599732279777527,
        0.029968954622745514,
        0.02980267070233822,
        0.029786041006445885,
        0.029764922335743904,
        0.029863595962524414,
        0.0298149436712265,
        0.029826387763023376,
        0.029773931950330734,
        0.029779236763715744,
        0.029820222407579422,
        0.029765887185931206,
        0.029764335602521896,
        0.029766017571091652,
        0.029754117131233215,
        0.02975280210375786,
        0.029757123440504074,
        0.029778512194752693,
        0.029764754697680473,
        0.029765650629997253,
        0.029766501858830452,
        0.029823405668139458,
        0.029806537553668022,
        0.029760312288999557,
        0.029792673885822296,
        0.02978181280195713,
        0.029768019914627075,
        0.029840050265192986,
        0.02996998466551304,
        0.02988036721944809,
        0.029842577874660492,
        0.02990039996802807
    ],
    "comments": {
        "created-by": "run_analogies.py",
        "args": {
            "model": "KoboldAI/OPT-6.7B-Erebus",
            "cache_dir": "./cache/",
            "tokenizer": "KoboldAI/OPT-6.7B-Erebus",
            "data_dir": "data/analogies/OPT6B/",
            "importance_results_dir": "rationalization_results/analogies/OPT6B_attention_last",
            "device": "cuda",
            "rationalization_config": "config/eva_attention_last.json",
            "input_num_ratio": 1,
            "logfolder": "logs/analogies/OPT6B_attention_last",
            "loglevel": 20
        },
        "time_elapsed": 0.04246973991394043
    }
}