{
    "$schema": "../docs/rationalization.schema.json",
    "id": 6,
    "input-text": [
        "</s>",
        "As",
        " soon",
        " as",
        " I",
        " arrived",
        " in",
        " Brazil",
        ",",
        " I",
        " checked",
        " into",
        " my",
        " hotel",
        " and",
        " took",
        " a",
        " long",
        " nap",
        ".",
        " (",
        "I",
        " had",
        " finally",
        " finished",
        " the",
        " book",
        " I",
        " was",
        " reading",
        " and",
        " it",
        " was",
        " amazing",
        ").",
        " I",
        " had",
        " to",
        " figure",
        " out",
        " the",
        " exchange",
        " rate",
        " to",
        " the",
        " local",
        " currency",
        ",",
        " which",
        " is",
        " apparently",
        " called",
        " the"
    ],
    "input-tokens": [
        2,
        1620,
        1010,
        25,
        38,
        2035,
        11,
        2910,
        6,
        38,
        7869,
        88,
        127,
        2303,
        8,
        362,
        10,
        251,
        16159,
        4,
        36,
        100,
        56,
        1747,
        1550,
        5,
        1040,
        38,
        21,
        2600,
        8,
        24,
        21,
        2770,
        322,
        38,
        56,
        7,
        1955,
        66,
        5,
        2081,
        731,
        7,
        5,
        400,
        2593,
        6,
        61,
        16,
        4100,
        373,
        5
    ],
    "target-text": " real",
    "target-token": 588,
    "rational-size": 5,
    "rational-positions": [
        0,
        51,
        52,
        47,
        34
    ],
    "rational-text": [
        "</s>",
        " called",
        " the",
        ",",
        ")."
    ],
    "rational-tokens": [
        2,
        373,
        5,
        6,
        322
    ],
    "importance-scores": [
        0.03374897688627243,
        0.01851254142820835,
        0.018481353297829628,
        0.01849023438990116,
        0.018485747277736664,
        0.018484016880393028,
        0.018525317311286926,
        0.0186812374740839,
        0.018575146794319153,
        0.018495896831154823,
        0.018485451117157936,
        0.01851305365562439,
        0.018491428345441818,
        0.018490277230739594,
        0.018536319956183434,
        0.018485428765416145,
        0.018515564501285553,
        0.01847173273563385,
        0.018479807302355766,
        0.01871483214199543,
        0.018529100343585014,
        0.018487123772501945,
        0.018488110974431038,
        0.01848008669912815,
        0.01848446950316429,
        0.018537068739533424,
        0.018504580482840538,
        0.018495159223675728,
        0.018500206992030144,
        0.018495680764317513,
        0.018548650667071342,
        0.01849425584077835,
        0.01850845292210579,
        0.01849241927266121,
        0.01881752349436283,
        0.018533814698457718,
        0.018514269962906837,
        0.018541721627116203,
        0.01850138232111931,
        0.018510406836867332,
        0.018581362441182137,
        0.018578344956040382,
        0.01863439939916134,
        0.018635576590895653,
        0.01865612342953682,
        0.018645113334059715,
        0.018797803670167923,
        0.018836377188563347,
        0.01865898072719574,
        0.018742665648460388,
        0.01870630867779255,
        0.01933959685266018,
        0.019058499485254288
    ],
    "comments": {
        "created-by": "run_analogies.py",
        "args": {
            "model": "facebook/opt-1.3b",
            "cache_dir": "./cache/",
            "tokenizer": "facebook/opt-1.3b",
            "data_dir": "data/analogies/OPT1B/",
            "importance_results_dir": "rationalization_results/analogies/OPT1B_attention",
            "device": "cuda",
            "rationalization_config": "config/eva_attention.json",
            "input_num_ratio": 1,
            "logfolder": "logs/analogies/OPT1B_attention",
            "loglevel": 20
        },
        "time_elapsed": 0.021889686584472656
    }
}