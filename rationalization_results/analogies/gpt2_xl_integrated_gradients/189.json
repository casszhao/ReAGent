{
    "$schema": "../docs/rationalization.schema.json",
    "id": 189,
    "input-text": [
        "I",
        " really",
        " wanted",
        " to",
        " buy",
        " the",
        " woman",
        ",",
        " more",
        " than",
        " I",
        " ever",
        " wanted",
        " to",
        " buy",
        " anything",
        " before",
        ".",
        " (",
        "I",
        " was",
        " also",
        " behind",
        " on",
        " my",
        " homework",
        ",",
        " but",
        " that",
        "'s",
        " another",
        " story",
        ").",
        " So",
        " I",
        " went",
        " to",
        " the",
        " store",
        " and",
        " asked",
        " if",
        " they",
        " had",
        " any"
    ],
    "input-tokens": [
        40,
        1107,
        2227,
        284,
        2822,
        262,
        2415,
        11,
        517,
        621,
        314,
        1683,
        2227,
        284,
        2822,
        1997,
        878,
        13,
        357,
        40,
        373,
        635,
        2157,
        319,
        616,
        26131,
        11,
        475,
        326,
        338,
        1194,
        1621,
        737,
        1406,
        314,
        1816,
        284,
        262,
        3650,
        290,
        1965,
        611,
        484,
        550,
        597
    ],
    "target-text": " women",
    "target-token": 1466,
    "rational-size": 5,
    "rational-positions": [
        6,
        4,
        0,
        1,
        2
    ],
    "rational-text": [
        " woman",
        " buy",
        "I",
        " really",
        " wanted"
    ],
    "rational-tokens": [
        2415,
        2822,
        40,
        1107,
        2227
    ],
    "importance-scores": [
        0.07375233620405197,
        0.05685056373476982,
        0.055054184049367905,
        0.025696827098727226,
        0.08865602314472198,
        0.036944929510354996,
        0.1293378472328186,
        0.007411979138851166,
        0.004656205885112286,
        0.010567905381321907,
        0.004970256704837084,
        0.009495860897004604,
        0.01001868024468422,
        0.004428636282682419,
        0.010961339809000492,
        0.008971893228590488,
        0.011374917812645435,
        0.008705208078026772,
        0.012895025312900543,
        0.009345619939267635,
        0.010055218823254108,
        0.00882445927709341,
        0.011663800105452538,
        0.0070375483483076096,
        0.010752794332802296,
        0.038538120687007904,
        0.0065067545510828495,
        0.010684964247047901,
        0.009773542173206806,
        0.01259882003068924,
        0.045263100415468216,
        0.018801895901560783,
        0.041351139545440674,
        0.018249038606882095,
        0.010319784283638,
        0.011139189824461937,
        0.0045561520382761955,
        0.006538850720971823,
        0.050206709653139114,
        0.003641592338681221,
        0.024314049631357193,
        0.010974072851240635,
        0.01335954386740923,
        0.01317264512181282,
        0.021580029278993607
    ],
    "comments": {
        "created-by": "run_analogies.py",
        "args": {
            "model": "gpt2-xl",
            "cache_dir": "cache/",
            "tokenizer": "gpt2-xl",
            "data_dir": "data/analogies/gpt2_xl/",
            "importance_results_dir": "rationalization_results/analogies/gpt2_xl_integrated_gradients",
            "device": "cuda",
            "rationalization_config": "config/eva_integrated_gradients.json",
            "input_num_ratio": 1,
            "logfolder": "logs/analogies/gpt2_xl_integrated_gradients",
            "loglevel": 20
        },
        "time_elapsed": 1.1108427047729492
    }
}