{
    "$schema": "../docs/rationalization.schema.json",
    "id": 52,
    "input-text": [
        "I",
        " really",
        " wanted",
        " to",
        " buy",
        " the",
        " pear",
        ",",
        " more",
        " than",
        " I",
        " ever",
        " wanted",
        " to",
        " buy",
        " anything",
        " before",
        ".",
        " (",
        "I",
        " was",
        " also",
        " behind",
        " on",
        " my",
        " homework",
        ",",
        " but",
        " that",
        "'s",
        " another",
        " story",
        ").",
        " So",
        " I",
        " went",
        " to",
        " the",
        " store",
        " and",
        " asked",
        " if",
        " they",
        " had",
        " any",
        " p"
    ],
    "input-tokens": [
        40,
        1107,
        2227,
        284,
        2822,
        262,
        25286,
        11,
        517,
        621,
        314,
        1683,
        2227,
        284,
        2822,
        1997,
        878,
        13,
        357,
        40,
        373,
        635,
        2157,
        319,
        616,
        26131,
        11,
        475,
        326,
        338,
        1194,
        1621,
        737,
        1406,
        314,
        1816,
        284,
        262,
        3650,
        290,
        1965,
        611,
        484,
        550,
        597,
        279
    ],
    "target-text": "ears",
    "target-token": 4127,
    "rational-size": 12,
    "rational-positions": [
        0,
        1,
        2,
        3,
        6,
        20,
        22,
        23,
        24,
        38,
        39,
        45
    ],
    "rational-text": [
        "I",
        " really",
        " wanted",
        " to",
        " pear",
        " was",
        " behind",
        " on",
        " my",
        " store",
        " and",
        " p"
    ],
    "rational-tokens": [
        40,
        1107,
        2227,
        284,
        25286,
        373,
        2157,
        319,
        616,
        3650,
        290,
        279
    ],
    "importance-scores": [
        3.357497278219819e-22,
        8.845586161282279e-19,
        8.582808665057352e-18,
        6.389975237661084e-35,
        6.741047546706872e-26,
        1.7116384418934948e-19,
        4.839401299250312e-06,
        1.0188435009127628e-14,
        6.436770655192511e-15,
        4.325514691436508e-14,
        6.338257559735643e-23,
        3.325192288191315e-23,
        1.5249174757285097e-29,
        1.3702890750799494e-14,
        9.640516872551278e-24,
        6.942275444289674e-31,
        1.6667850925692503e-27,
        1.3698485243105319e-18,
        2.3276394563208223e-19,
        2.159208686531766e-25,
        4.213689408061327e-06,
        1.0836987518829587e-28,
        1.0196883337698637e-08,
        5.572062993300375e-13,
        0.09999094903469086,
        7.674670128194957e-19,
        8.337500306853426e-15,
        2.525503126795038e-19,
        8.525842847079015e-30,
        2.539807419663069e-23,
        1.3186283039163554e-26,
        4.850801379254587e-22,
        5.984818718907314e-19,
        7.694649851217095e-30,
        1.3322087329867953e-23,
        8.811514343697036e-15,
        9.477281181447732e-16,
        2.624091423583995e-19,
        0.0005490417825058103,
        4.2874948287995664e-17,
        1.8637701792739456e-30,
        1.0137851141194777e-18,
        7.668327112013623e-33,
        2.2503806644247216e-23,
        5.198780805209997e-34,
        0.899450957775116
    ],
    "comments": {
        "created-by": "run_analogies.py",
        "args": {
            "model": "EleutherAI/gpt-j-6b",
            "cache_dir": "cache/",
            "tokenizer": "EleutherAI/gpt-j-6b",
            "data_dir": "data/analogies/gpt6b",
            "importance_results_dir": "rationalization_results/analogies/gpt6b_ours/top3_replace0.3_max3000_batch10",
            "device": "cuda",
            "rationalization_config": "config//top3_replace0.3_max3000_batch10.json",
            "input_num_ratio": 1.0,
            "logfolder": "logs/analogies/gpt6b_ours/top3_replace0.3_max3000_batch10",
            "loglevel": 20
        },
        "time_elapsed": 79.24758648872375,
        "separate_rational": [
            [
                " p",
                " behind",
                " was",
                " pear",
                " on"
            ],
            [
                " p",
                "I",
                " really",
                " wanted",
                " to"
            ],
            [
                " my",
                " pear",
                " was",
                " p",
                " than"
            ],
            [
                " p",
                " store",
                ",",
                " I",
                " pear"
            ],
            [
                " p",
                " the",
                " I",
                "I",
                " pear"
            ],
            [
                " p",
                " and",
                " on",
                " pear",
                " also"
            ],
            [
                " p",
                " store",
                " behind",
                " story",
                " pear"
            ],
            [
                " p",
                "I",
                " really",
                " wanted",
                " to"
            ],
            [
                " p",
                " pear",
                " anything",
                " and",
                "I"
            ],
            [
                " p",
                " had",
                " my",
                " pear",
                " So"
            ]
        ]
    }
}