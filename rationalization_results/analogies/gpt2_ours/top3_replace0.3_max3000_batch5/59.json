{
    "$schema": "../docs/rationalization.schema.json",
    "id": 59,
    "input-text": [
        "As",
        " soon",
        " as",
        " I",
        " arrived",
        " in",
        " Japan",
        ",",
        " I",
        " checked",
        " into",
        " my",
        " hotel",
        " and",
        " took",
        " a",
        " long",
        " nap",
        ".",
        " (",
        "I",
        " had",
        " finally",
        " finished",
        " the",
        " book",
        " I",
        " was",
        " reading",
        " and",
        " it",
        " was",
        " amazing",
        ").",
        " I",
        " had",
        " to",
        " figure",
        " out",
        " the",
        " exchange",
        " rate",
        " to",
        " the",
        " local",
        " currency",
        ",",
        " which",
        " is",
        " apparently",
        " called",
        " the"
    ],
    "input-tokens": [
        1722,
        2582,
        355,
        314,
        5284,
        287,
        2869,
        11,
        314,
        10667,
        656,
        616,
        7541,
        290,
        1718,
        257,
        890,
        25422,
        13,
        357,
        40,
        550,
        3443,
        5201,
        262,
        1492,
        314,
        373,
        3555,
        290,
        340,
        373,
        4998,
        737,
        314,
        550,
        284,
        3785,
        503,
        262,
        5163,
        2494,
        284,
        262,
        1957,
        7395,
        11,
        543,
        318,
        5729,
        1444,
        262
    ],
    "target-text": " yen",
    "target-token": 28808,
    "rational-size": 9,
    "rational-positions": [
        6,
        31,
        34,
        39,
        45,
        46,
        49,
        50,
        51
    ],
    "rational-text": [
        " Japan",
        " was",
        " I",
        " the",
        " currency",
        ",",
        " apparently",
        " called",
        " the"
    ],
    "rational-tokens": [
        2869,
        373,
        314,
        262,
        7395,
        11,
        5729,
        1444,
        262
    ],
    "importance-scores": [
        2.790975433596766e-35,
        0.0,
        0.0,
        2.075610235798302e-37,
        1.901720042021316e-27,
        4.759931822644287e-16,
        2.2324350368307933e-07,
        2.1946912340500993e-37,
        5.994544379297614e-33,
        0.0,
        5.474451441619217e-15,
        8.407790785948902e-45,
        1.1286216886615408e-17,
        1.0598232386224152e-36,
        3.535548823058839e-34,
        0.0,
        2.5783891743576634e-42,
        3.8511513595545136e-32,
        1.600072624763728e-33,
        1.7039789326189776e-42,
        0.0,
        1.610981760034061e-39,
        5.186365364491081e-39,
        2.7713419550155726e-32,
        0.0,
        0.0,
        2.0755569078153688e-20,
        0.0,
        2.6429108300006406e-24,
        1.6111251093048204e-29,
        8.749795070204952e-35,
        2.477887600080779e-36,
        3.644146418342364e-28,
        5.528767693205546e-29,
        0.0,
        3.338118118353505e-35,
        2.120470586028478e-34,
        1.4614141684443517e-41,
        0.0,
        0.0,
        5.6071182096935025e-30,
        8.017840757037824e-29,
        8.127531093083939e-44,
        6.848684013116019e-23,
        1.354606922982575e-32,
        3.976317227414629e-14,
        0.0,
        9.08916561702853e-25,
        1.4299159971255618e-31,
        1.6047459177538684e-29,
        0.9999997615814209,
        1.5271639043530262e-22
    ],
    "comments": {
        "created-by": "run_analogies.py",
        "args": {
            "model": "gpt2-medium",
            "cache_dir": "cache/",
            "tokenizer": "gpt2-medium",
            "data_dir": "data/analogies/gpt2/",
            "importance_results_dir": "rationalization_results/analogies/gpt2_ours/top3_replace0.3_max3000_batch5",
            "device": "cuda",
            "rationalization_config": "config/gpt2/top3_replace0.3_max3000_batch5.json",
            "input_num_ratio": 1.0,
            "logfolder": "logs/analogies/gpt2_ours/top3_replace0.3_max3000_batch5",
            "loglevel": 20
        },
        "time_elapsed": 172.55292057991028,
        "separate_rational": [
            [
                " called",
                " Japan",
                " currency",
                " into",
                " in",
                " hotel",
                " I",
                " the"
            ],
            [
                " called",
                " Japan",
                " I",
                " the",
                " soon",
                " was",
                " reading",
                " the"
            ],
            [
                " the",
                " local",
                " apparently",
                ",",
                " called",
                " I",
                " the",
                " as"
            ],
            [
                " amazing",
                " to",
                " apparently",
                " called",
                " the",
                ",",
                " was",
                " my"
            ],
            [
                " currency",
                " rate",
                " finally",
                " called",
                " is",
                " Japan",
                " the",
                " had"
            ]
        ]
    }
}