{
    "$schema": "../docs/rationalization.schema.json",
    "id": 58,
    "input-text": [
        "As",
        " soon",
        " as",
        " I",
        " arrived",
        " in",
        " Europe",
        ",",
        " I",
        " checked",
        " into",
        " my",
        " hotel",
        " and",
        " took",
        " a",
        " long",
        " nap",
        ".",
        " (",
        "I",
        " had",
        " finally",
        " finished",
        " the",
        " book",
        " I",
        " was",
        " reading",
        " and",
        " it",
        " was",
        " amazing",
        ").",
        " I",
        " had",
        " to",
        " figure",
        " out",
        " the",
        " exchange",
        " rate",
        " to",
        " the",
        " local",
        " currency",
        ",",
        " which",
        " is",
        " apparently",
        " called",
        " the"
    ],
    "input-tokens": [
        1722,
        2582,
        355,
        314,
        5284,
        287,
        2031,
        11,
        314,
        10667,
        656,
        616,
        7541,
        290,
        1718,
        257,
        890,
        25422,
        13,
        357,
        40,
        550,
        3443,
        5201,
        262,
        1492,
        314,
        373,
        3555,
        290,
        340,
        373,
        4998,
        737,
        314,
        550,
        284,
        3785,
        503,
        262,
        5163,
        2494,
        284,
        262,
        1957,
        7395,
        11,
        543,
        318,
        5729,
        1444,
        262
    ],
    "target-text": " Euro",
    "target-token": 1898,
    "rational-size": 6,
    "rational-positions": [
        10,
        14,
        25,
        45,
        47,
        51
    ],
    "rational-text": [
        " into",
        " took",
        " book",
        " currency",
        " which",
        " the"
    ],
    "rational-tokens": [
        656,
        1718,
        1492,
        7395,
        543,
        262
    ],
    "importance-scores": [
        1.924293016597467e-11,
        3.741977685150317e-16,
        4.901524519596023e-08,
        8.589816704643023e-16,
        3.183483104945034e-20,
        1.7126894402007256e-27,
        0.326215535402298,
        4.1748149837559218e-22,
        1.3793709942118017e-25,
        6.700900477206146e-21,
        4.236597703766165e-07,
        2.1926293334492782e-26,
        3.459633422244074e-28,
        4.6410453063104595e-14,
        0.6737673282623291,
        3.841641573161583e-19,
        5.3067236963811226e-33,
        2.766521809396975e-28,
        1.7264752808946815e-15,
        1.33200889479798e-26,
        5.058271621814347e-31,
        2.8977403701384297e-14,
        1.506306926889962e-22,
        9.498231085560189e-17,
        7.661913948494109e-17,
        1.624345895834267e-05,
        9.461427101274732e-41,
        8.802850247529509e-23,
        2.2363780965983793e-11,
        1.2972588816697025e-24,
        1.179551480217839e-16,
        2.40730008683038e-23,
        9.574281486882006e-20,
        2.3955582709093998e-21,
        6.47332687453428e-10,
        3.795160226590697e-20,
        1.2672778408259755e-13,
        6.847817182562077e-18,
        4.152366096819035e-25,
        3.974919428975781e-24,
        1.70196870053275e-20,
        5.885476643687749e-23,
        1.145277334100275e-23,
        1.550081868417763e-10,
        1.6748209195954758e-21,
        3.110858237675984e-09,
        2.608549200706135e-22,
        1.9514245774843175e-09,
        5.802925312850277e-15,
        5.5336932349641916e-27,
        6.842340088854119e-16,
        3.9206045698847447e-07
    ],
    "comments": {
        "created-by": "run_analogies.py",
        "args": {
            "model": "gpt2-medium",
            "cache_dir": "cache/",
            "tokenizer": "gpt2-medium",
            "data_dir": "data/analogies/gpt2/",
            "importance_results_dir": "rationalization_results/analogies/gpt2_ours/top3_replace0.3_max3000_batch3",
            "device": "cuda",
            "rationalization_config": "config/gpt2/top3_replace0.3_max3000_batch3.json",
            "input_num_ratio": 1.0,
            "logfolder": "logs/analogies/gpt2_ours/top3_replace0.3_max3000_batch3",
            "loglevel": 20
        },
        "time_elapsed": 128.75507426261902,
        "separate_rational": [
            [
                " called",
                " into",
                " checked",
                "I",
                " currency",
                " book",
                " took",
                " it"
            ],
            [
                " to",
                " had",
                " which",
                " finished",
                ").",
                " hotel",
                " arrived",
                " the"
            ],
            [
                " took",
                " Europe",
                " book",
                " into",
                " the",
                " as",
                " currency",
                " which"
            ]
        ]
    }
}