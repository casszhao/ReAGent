{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook requires a Google Colab **T4 runtime**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/googlecolab/colabtools/issues/3409\n",
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/casszhao/ReAGent.git\n",
    "!pip install -r ReAGent/requirments.txt\n",
    "!python ReAGent/setup_nltk.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Global configuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\"\n",
    "torch.manual_seed(42)\n",
    "torch.use_deterministic_algorithms(True, warn_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2-medium\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Configure prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_inference_length = 5\n",
    "input_string = \"Super Mario Land is a game that developed by\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Run prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(input_string, return_tensors='pt')['input_ids'][0].to(model.device)\n",
    "generated_ids = model.generate(input_ids=torch.unsqueeze(input_ids, 0), max_length=(input_ids.shape[0] + max_inference_length), do_sample=False)[0]\n",
    "generated_texts = [ tokenizer.decode(token) for token in generated_ids ]\n",
    "print(f'generated full sequence --> {generated_texts}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Rationalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Construct rationalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ReAGent.src.rationalization.rationalizer.aggregate_rationalizer import AggregateRationalizer\n",
    "from ReAGent.src.rationalization.rationalizer.importance_score_evaluator.delta_prob import DeltaProbImportanceScoreEvaluator\n",
    "from ReAGent.src.rationalization.rationalizer.stopping_condition_evaluator.top_k import TopKStoppingConditionEvaluator\n",
    "from ReAGent.src.rationalization.rationalizer.token_replacement.token_replacer.uniform import UniformTokenReplacer\n",
    "from ReAGent.src.rationalization.rationalizer.token_replacement.token_sampler.postag import POSTagTokenSampler\n",
    "\n",
    "rational_size = 5\n",
    "rational_size_ratio = None\n",
    "\n",
    "token_sampler = POSTagTokenSampler(tokenizer=tokenizer, device=device)\n",
    "\n",
    "stopping_condition_evaluator = TopKStoppingConditionEvaluator(\n",
    "    model=model, \n",
    "    token_sampler=token_sampler, \n",
    "    top_k=3, \n",
    "    top_n=rational_size, \n",
    "    top_n_ratio=rational_size_ratio, \n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "importance_score_evaluator = DeltaProbImportanceScoreEvaluator(\n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    token_replacer=UniformTokenReplacer(\n",
    "        token_sampler=token_sampler, \n",
    "        ratio=0.3\n",
    "    ),\n",
    "    stopping_condition_evaluator=stopping_condition_evaluator,\n",
    "    max_steps=3000\n",
    ")\n",
    "\n",
    "rationalizer = AggregateRationalizer(\n",
    "    importance_score_evaluator=importance_score_evaluator,\n",
    "    batch_size=8,\n",
    "    overlap_threshold=2,\n",
    "    overlap_strict_pos=True,\n",
    "    top_n=rational_size, \n",
    "    top_n_ratio=rational_size_ratio\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Run rationalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rationalize each generated token\n",
    "\n",
    "importance_scores = []\n",
    "importance_score_map = torch.zeros([generated_ids.shape[0] - input_ids.shape[0], generated_ids.shape[0] - 1], device=device)\n",
    "\n",
    "for target_pos in torch.arange(input_ids.shape[0], generated_ids.shape[0]):\n",
    "    \n",
    "    # extract target\n",
    "    target_id = generated_ids[target_pos]\n",
    "\n",
    "    # rationalization\n",
    "    pos_rational = rationalizer.rationalize(torch.unsqueeze(generated_ids[:target_pos], 0), torch.unsqueeze(target_id, 0))[0]\n",
    "\n",
    "    ids_rational = generated_ids[pos_rational]\n",
    "    text_rational = [ tokenizer.decode([id_rational]) for id_rational in ids_rational ]\n",
    "\n",
    "    importance_score_map[target_pos - input_ids.shape[0], :target_pos] = rationalizer.mean_important_score\n",
    "\n",
    "    print(f'{target_pos + 1} / {generated_ids.shape[0]}')\n",
    "    print(f'Target word     --> {tokenizer.decode(target_id)}', )\n",
    "    print(f\"Rational pos    --> {pos_rational}\")\n",
    "    print(f\"Rational text   --> {text_rational}\")\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Visualize rationalization results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn\n",
    "seaborn.set(rc={ 'figure.figsize': (30, 10) })\n",
    "s = seaborn.heatmap(\n",
    "    importance_score_map.cpu(), \n",
    "    xticklabels=generated_texts[:-1], \n",
    "    yticklabels=generated_texts[input_ids.shape[0]:], \n",
    "    annot=True, \n",
    "    square=True)\n",
    "s.set_xlabel('Importance distribution')\n",
    "s.set_ylabel('Target')\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Configure evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_stride = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ReAGent.src.evaluator.soft_norm_sufficiency import SoftNormalizedSufficiencyEvaluator\n",
    "from ReAGent.src.evaluation.evaluator.soft_norm_comprehensiveness import SoftNormalizedComprehensivenessEvaluator\n",
    "soft_norm_suff_evaluator = SoftNormalizedSufficiencyEvaluator(model)\n",
    "soft_norm_comp_evaluator = SoftNormalizedComprehensivenessEvaluator(model)\n",
    "\n",
    "source_soft_ns_all = []\n",
    "source_soft_nc_all = []\n",
    "random_soft_ns_all = []\n",
    "random_soft_nc_all = []\n",
    "target_token_all = []\n",
    "\n",
    "table_details = [ [\"target_pos\", \"target_token\", \"source_soft_ns\", \"source_soft_nc\", \"rand_soft_ns\", \"rand_soft_nc\"] ]\n",
    "\n",
    "for target_pos in torch.arange(input_ids.shape[0], generated_ids.shape[0], metric_stride):\n",
    "\n",
    "    target_token = tokenizer.decode(generated_ids[target_pos])\n",
    "    target_token_all.append(target_token)\n",
    "\n",
    "    input_ids_step = torch.unsqueeze(generated_ids[:target_pos], 0)\n",
    "    target_id_step = torch.unsqueeze(generated_ids[target_pos], 0)\n",
    "    importance_score_step = torch.unsqueeze(importance_score_map[target_pos - input_ids.shape[0], :target_pos], 0)\n",
    "    random_importance_score_step = torch.softmax(torch.rand(importance_score_step.shape, device=device), dim=-1)\n",
    "\n",
    "    # compute Soft-NS and Soft-NC on source importance score\n",
    "\n",
    "    source_soft_ns_step = soft_norm_suff_evaluator.evaluate(input_ids_step, target_id_step, importance_score_step)\n",
    "    source_soft_ns_all.append(source_soft_ns_step)\n",
    "\n",
    "    source_soft_nc_step = soft_norm_comp_evaluator.evaluate(input_ids_step, target_id_step, importance_score_step)\n",
    "    source_soft_nc_all.append(source_soft_nc_step)\n",
    "\n",
    "    # compute Soft-NS and Soft-NC on random importance score\n",
    "\n",
    "    random_soft_ns_step = soft_norm_suff_evaluator.evaluate(input_ids_step, target_id_step, random_importance_score_step)\n",
    "    random_soft_ns_all.append(random_soft_ns_step)\n",
    "\n",
    "    random_soft_nc_step = soft_norm_comp_evaluator.evaluate(input_ids_step, target_id_step, random_importance_score_step)\n",
    "    random_soft_nc_all.append(random_soft_nc_step)\n",
    "\n",
    "    table_details.append([\n",
    "        target_pos.item() + 1, target_token, \n",
    "        f\"{source_soft_ns_step.item():.3f}\", f\"{source_soft_nc_step.item():.3f}\", \n",
    "        f\"{random_soft_ns_step.item():.3f}\", f\"{random_soft_nc_step.item():.3f}\", \n",
    "        # metric_soft_ns_step.item(), metric_soft_nc_step.item()\n",
    "        ])\n",
    "    print(f\"target_pos: {target_pos + 1}, target_token: {target_token}, Source Soft-NS: {source_soft_ns_step}, Source Soft-NC: {source_soft_nc_step}, Random Soft-NS: {random_soft_ns_step}, Random Soft-NC: {random_soft_nc_step}\")\n",
    "\n",
    "# compute metrics on Soft-NS and Soft-NC\n",
    "\n",
    "metric_soft_ns = torch.log(torch.sum(torch.tensor(source_soft_ns_all, device=device)) / torch.sum(torch.tensor(random_soft_ns_all, device=device)))\n",
    "metric_soft_nc = torch.log(torch.sum(torch.tensor(source_soft_nc_all, device=device)) / torch.sum(torch.tensor(random_soft_nc_all, device=device)))\n",
    "\n",
    "print(f\"metric_soft_ns: {metric_soft_ns}, metric_soft_nc: {metric_soft_nc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Show metrics in tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tabulate\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "table_details_html = tabulate.tabulate(table_details, tablefmt='html')\n",
    "\n",
    "display(HTML(table_details_html))\n",
    "\n",
    "table_mean = [\n",
    "        [ \"target_tokens\", \"metric_soft_ns\", \"metric_soft_nc\" ],\n",
    "        [ \"$\".join(target_token_all), f\"{metric_soft_ns.item():.3f}\", f\"{metric_soft_nc.item():.3f}\" ]\n",
    "    ]\n",
    "\n",
    "table_mean_html = tabulate.tabulate(table_mean, tablefmt='html')\n",
    "\n",
    "display(HTML(table_mean_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Save results to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('notebook_details.csv', 'w', newline='') as csvfile:\n",
    "    csvWriter = csv.writer(csvfile)\n",
    "    csvWriter.writerows(table_details)\n",
    "\n",
    "with open('notebook_mean.csv', 'w', newline='') as csvfile:\n",
    "    csvWriter = csv.writer(csvfile)\n",
    "    csvWriter.writerows(table_mean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
